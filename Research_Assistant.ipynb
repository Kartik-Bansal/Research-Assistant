{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75uQqNa3PyTj"
   },
   "source": [
    "# Building a QA System with BERT on Wikipedia\n",
    "> A high-level code walk-through of an IR-based QA system with PyTorch and Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5Uoou3CPyTm"
   },
   "outputs": [],
   "source": [
    "# collapse-hide \n",
    "\n",
    "# line 69 of `run_squad.py` script shows why you might need to install \n",
    "# tensorboardX if you have an older version of torch\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wExAJnAbPyTq"
   },
   "source": [
    "Conversely, if you're working in Colab, you can run the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vRsi62cFPyTr",
    "outputId": "e8af6604-d3bb-45b2-8dbb-d16968aebefc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: wikipedia in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (2.23.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch  torchvision -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install transformers\n",
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilYOjditPyTu"
   },
   "source": [
    "## Fine-tuning a Transformer model for Question Answering\n",
    "\n",
    "To train a Transformer for QA with Hugging Face, we'll need\n",
    "1. to pick a specific model architecture,\n",
    "2. a QA dataset, and\n",
    "3. the training script.\n",
    "\n",
    "With these three things in hand we'll then walk through the fine-tuning process. \n",
    "\n",
    "### 1. Pick a Model\n",
    "Not every Transformer architecture lends itself naturally to the task of question answering. For example, GPT does not do QA; similarly BERT does not do machine translation.  HF identifies the following model types for the QA task: \n",
    "\n",
    "- BERT\n",
    "- distilBERT \n",
    "- ALBERT\n",
    "- RoBERTa\n",
    "- XLNet\n",
    "- XLM\n",
    "- FlauBERT\n",
    "\n",
    "\n",
    "We'll stick with the now-classic BERT model in this notebook, but feel free to try out some others (we will - and we'll let you know when we do). Next up: a training set. \n",
    "\n",
    "\n",
    "### 2. QA dataset: SQuAD \n",
    "One of the most canonical datasets for QA is the Stanford Question Answering Dataset, or SQuAD, which comes in two flavors: SQuAD 1.1 and SQuAD 2.0. These reading comprehension datasets consist of questions posed on a set of Wikipedia articles, where the answer to every question is a segment (or span) of the corresponding passage. In SQuAD 1.1, all questions have an answer in the corresponding passage. SQuAD 2.0 steps up the difficulty by including questions that cannot be answered by the provided passage. \n",
    "\n",
    "The following code will download the specified version of SQuAD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zoVdirEHPyTu",
    "outputId": "903c0f22-fd1b-4d48-e5db-865946096cae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DATA_DIR=./data/squad\n",
      "--2021-04-26 18:19:12--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
      "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
      "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 42123633 (40M) [application/json]\n",
      "Saving to: ‘./data/squad/train-v2.0.json.1’\n",
      "\n",
      "train-v2.0.json.1   100%[===================>]  40.17M  66.3MB/s    in 0.6s    \n",
      "\n",
      "2021-04-26 18:19:13 (66.3 MB/s) - ‘./data/squad/train-v2.0.json.1’ saved [42123633/42123633]\n",
      "\n",
      "--2021-04-26 18:19:14--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
      "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
      "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4370528 (4.2M) [application/json]\n",
      "Saving to: ‘./data/squad/dev-v2.0.json.1’\n",
      "\n",
      "dev-v2.0.json.1     100%[===================>]   4.17M  22.0MB/s    in 0.2s    \n",
      "\n",
      "2021-04-26 18:19:14 (22.0 MB/s) - ‘./data/squad/dev-v2.0.json.1’ saved [4370528/4370528]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set path with magic\n",
    "%env DATA_DIR=./data/squad \n",
    "\n",
    "# download the data\n",
    "def download_squad(version=1):\n",
    "    if version == 1:\n",
    "        !wget -P $DATA_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
    "        !wget -P $DATA_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
    "    else:\n",
    "        !wget -P $DATA_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
    "        !wget -P $DATA_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
    "            \n",
    "download_squad(version=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FT6OEEHDPyUF"
   },
   "source": [
    "## Using a pre-fine-tuned model from the Hugging Face repository\n",
    "\n",
    "![](https://github.com/fastforwardlabs/ff14_blog/blob/master/_notebooks/my_icons/HF_repo.png?raw=1)\n",
    "\n",
    "\n",
    "Each of these links provides explicit code for using the model, and, in some cases, information on how it was trained and what results were achieved. Let's load one of these pre-fine-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfkQquExPyUF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# executing these commands for the first time initiates a download of the \n",
    "# model weights to ~/.cache/torch/transformers/\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\") \n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oRCvXFaPyUI"
   },
   "source": [
    "## Let's try our model!\n",
    "\n",
    "Whether you fine-tuned your own or used a pre-fine-tuned model, it's time to play with it! There are three steps to QA: \n",
    "1. tokenize the input\n",
    "2. obtain model scores\n",
    "3. get the answer span\n",
    "\n",
    "These steps are discussed in detail in the HF [Transformer Notebooks](https://huggingface.co/transformers/notebooks.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YqkNZqmZNhqE",
    "outputId": "2478ea4d-b376-4337-9d2a-82e5310d7665"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.7/dist-packages (20201018)\n",
      "Requirement already satisfied: chardet; python_version > \"3.0\" in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (3.0.4)\n",
      "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (2.3.0)\n",
      "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (3.4.7)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six) (2.20)\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIn_UOyQNwFd"
   },
   "outputs": [],
   "source": [
    "text = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "n5Sz3EQPNq9k",
    "outputId": "0438307a-69a6-4845-9d5b-9837746db4f0"
   },
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer\n",
    "for page_layout in extract_pages(\"sample.pdf\"):\n",
    "    for element in page_layout:\n",
    "        if isinstance(element, LTTextContainer):\n",
    "            text+=element.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXcNGO6RPyUL"
   },
   "outputs": [],
   "source": [
    "question = \"What is atmosphere\"\n",
    "\n",
    "context = text\n",
    "# 1. TOKENIZE THE INPUT\n",
    "# note: if you don't include return_tensors='pt' you'll get a list of lists which is easier for \n",
    "# exploration but you cannot feed that into a model. \n",
    "inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\") \n",
    "\n",
    "# 2. OBTAIN MODEL SCORES\n",
    "# the AutoModelForQuestionAnswering class includes a span predictor on top of the model. \n",
    "# the model returns answer start and end scores for each word in the text\n",
    "answer = model(**inputs)\n",
    "answer_start_scores = answer[0]\n",
    "answer_end_scores = answer[1]\n",
    "answer_start = torch.argmax(answer_start_scores)  # get the most likely beginning of answer with the argmax of the score\n",
    "answer_end = torch.argmax(answer_end_scores) + 1  # get the most likely end of answer with the argmax of the score\n",
    "\n",
    "# 3. GET THE ANSWER SPAN\n",
    "# once we have the most likely start and end tokens, we grab all the tokens between them\n",
    "# and convert tokens back to words!\n",
    "tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ulbQBK8PyUN"
   },
   "source": [
    "# QA on Wikipedia pages\n",
    "I tried our model on a question paired with a short passage, but what if we want to retrieve an answer from a longer document? A typical Wikipedia page is much longer than the example above, and we need to do a bit of massaging before we can use our model on longer contexts. \n",
    "\n",
    "Let's start by pulling up a Wikipedia page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sCpRPl8dNXGb",
    "outputId": "a63fe7e5-2b85-42a8-8cf7-aec8fbd97d4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (2.23.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HXan9nNfPyUN",
    "outputId": "d88d9bf7-d6a3-4e66-d1e4-14cabc84a938",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia search results for our question:\n",
      "\n",
      "['Albatross',\n",
      " 'List of largest birds',\n",
      " 'Black-browed albatross',\n",
      " 'Mollymawk',\n",
      " 'List of birds by flight speed',\n",
      " 'Argentavis',\n",
      " 'Largest body part',\n",
      " 'Pterosaur',\n",
      " 'Aspect ratio (aeronautics)',\n",
      " 'Pteranodon']\n",
      "\n",
      "The Albatross Wikipedia article contains 38470 characters.\n"
     ]
    }
   ],
   "source": [
    "import wikipedia as wiki\n",
    "import pprint as pp\n",
    "\n",
    "question = 'What is the wingspan of an albatross?'\n",
    "\n",
    "results = wiki.search(question)\n",
    "print(\"Wikipedia search results for our question:\\n\")\n",
    "pp.pprint(results)\n",
    "\n",
    "page = wiki.page(results[0])\n",
    "text = page.content\n",
    "print(f\"\\nThe {results[0]} Wikipedia article contains {len(text)} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_gBvacPPyUS",
    "outputId": "74de443c-3f15-450e-fb1e-1bc0f3856cc1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8890 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This translates into 8890 tokens.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode_plus(question, text, return_tensors='pt')\n",
    "print(f\"This translates into {len(inputs['input_ids'][0])} tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbFnMEdvPyUU"
   },
   "source": [
    "The tokenizer takes the input as text and returns tokens. In general, tokenizers convert words or pieces of words into a model-ingestible format. The specific tokens and format are dependent on the type of model. For example, BERT tokenizes words differently from RoBERTa, so be sure to always use the associated tokenizer appropriate for your model.\n",
    "\n",
    "In this case, the tokenizer converts our input text into 8824 tokens, but this far exceeds the maximum number of tokens that can be fed to the model at one time. Most BERT-esque models can only accept 512 tokens at once, thus the (somewhat confusing) warning above (how is 10 > 512?). This means we'll have to split our input into chunks and each chunk must not exceed 512 tokens in total. \n",
    "\n",
    "When working with Question Answering, it's crucial that each chunk follows this format:\n",
    "\n",
    "[CLS] question tokens [SEP] context tokens [SEP]\n",
    "\n",
    "This means that, for each segment of a Wikipedia article, we must prepend the original question, followed by the next \"chunk\" of article tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JKNRLv1gPyUV",
    "outputId": "fb5e9ef9-3efd-422f-bd75-6eccf2cfdc10",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question consists of 12 tokens.\n",
      "Each chunk will contain 497 tokens of the Wikipedia article.\n"
     ]
    }
   ],
   "source": [
    "# time to chunk!\n",
    "from collections import OrderedDict\n",
    "\n",
    "# identify question tokens (token_type_ids = 0)\n",
    "qmask = inputs['token_type_ids'].lt(1)\n",
    "qt = torch.masked_select(inputs['input_ids'], qmask)\n",
    "print(f\"The question consists of {qt.size()[0]} tokens.\")\n",
    "\n",
    "chunk_size = model.config.max_position_embeddings - qt.size()[0] - 1 # the \"-1\" accounts for\n",
    "# having to add a [SEP] token to the end of each chunk\n",
    "print(f\"Each chunk will contain {chunk_size - 2} tokens of the Wikipedia article.\")\n",
    "\n",
    "# create a dict of dicts; each sub-dict mimics the structure of pre-chunked model input\n",
    "chunked_input = OrderedDict()\n",
    "for k,v in inputs.items():\n",
    "    q = torch.masked_select(v, qmask)\n",
    "    c = torch.masked_select(v, ~qmask)\n",
    "    chunks = torch.split(c, chunk_size)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if i not in chunked_input:\n",
    "            chunked_input[i] = {}\n",
    "\n",
    "        thing = torch.cat((q, chunk))\n",
    "        if i != len(chunks)-1:\n",
    "            if k == 'input_ids':\n",
    "                thing = torch.cat((thing, torch.tensor([102])))\n",
    "            else:\n",
    "                thing = torch.cat((thing, torch.tensor([1])))\n",
    "\n",
    "        chunked_input[i][k] = torch.unsqueeze(thing, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wd0GlCg6PyUX",
    "outputId": "090ea30a-f167-45bf-917e-cb9eff0bd5a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in chunk 0: 512\n",
      "Number of tokens in chunk 1: 512\n",
      "Number of tokens in chunk 2: 512\n",
      "Number of tokens in chunk 3: 512\n",
      "Number of tokens in chunk 4: 512\n",
      "Number of tokens in chunk 5: 512\n",
      "Number of tokens in chunk 6: 512\n",
      "Number of tokens in chunk 7: 512\n",
      "Number of tokens in chunk 8: 512\n",
      "Number of tokens in chunk 9: 512\n",
      "Number of tokens in chunk 10: 512\n",
      "Number of tokens in chunk 11: 512\n",
      "Number of tokens in chunk 12: 512\n",
      "Number of tokens in chunk 13: 512\n",
      "Number of tokens in chunk 14: 512\n",
      "Number of tokens in chunk 15: 512\n",
      "Number of tokens in chunk 16: 512\n",
      "Number of tokens in chunk 17: 407\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(chunked_input.keys())):\n",
    "    print(f\"Number of tokens in chunk {i}: {len(chunked_input[i]['input_ids'].tolist()[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPi1UNRePyUb"
   },
   "source": [
    "Each of these chunks (except for the last one) has the following structure: \n",
    "\n",
    "[CLS], 12 question tokens, [SEP], 497 tokens of the Wikipedia article, [SEP] token = 512 tokens\n",
    "\n",
    "Each of these chunks can now be fed to the model without causing indexing errors. We'll get an \"answer\" for each chunk; however, not all answers are useful, since not every segment of a Wikipedia article is informative for our question. The model will return the [CLS] token when it determines that the context does not contain an answer to the question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ABn4iER0PyUb",
    "outputId": "e6666867-4263-4bc4-af17-319a37dc69bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " /  / \n"
     ]
    }
   ],
   "source": [
    "def convert_ids_to_string(tokenizer, input_ids):\n",
    "    return tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids))\n",
    "\n",
    "answer = ''\n",
    "\n",
    "# now we iterate over our chunks, looking for the best answer from each chunk\n",
    "for _, chunk in chunked_input.items():\n",
    "    a = model(**chunk)\n",
    "    answer_start_scores = a[0]\n",
    "    answer_end_scores = a[1]\n",
    "\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    ans = convert_ids_to_string(tokenizer, chunk['input_ids'][0][answer_start:answer_end])\n",
    "    \n",
    "    # if the ans == [CLS] then the model did not find a real answer in this chunk\n",
    "    if ans != '[CLS]':\n",
    "        answer += ans + \" / \"\n",
    "        \n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3-nx8WbPyUd"
   },
   "source": [
    "# Putting it all together\n",
    "\n",
    "Let's recap. We've essentially built a simple IR-based QA system! We're using `wikipedia`'s search engine to return a list of candidate documents that we then feed into our document reader (in this case, BERT fine-tuned on SQuAD 2.0). Let's make our code easier to read and more self-contained by packaging the document reader into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NpZLrUt-PyUe"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "\n",
    "class DocumentReader:\n",
    "    def __init__(self, pretrained_model_name_or_path='bert-large-uncased'):\n",
    "        self.READER_PATH = pretrained_model_name_or_path\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.READER_PATH)\n",
    "        self.model = AutoModelForQuestionAnswering.from_pretrained(self.READER_PATH)\n",
    "        self.max_len = self.model.config.max_position_embeddings\n",
    "        self.chunked = False\n",
    "\n",
    "    def tokenize(self, question, text):\n",
    "        self.inputs = self.tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "        self.input_ids = self.inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "        if len(self.input_ids) > self.max_len:\n",
    "            self.inputs = self.chunkify()\n",
    "            self.chunked = True\n",
    "\n",
    "    def chunkify(self):\n",
    "        \"\"\" \n",
    "        Break up a long article into chunks that fit within the max token\n",
    "        requirement for that Transformer model. \n",
    "\n",
    "        Calls to BERT / RoBERTa / ALBERT require the following format:\n",
    "        [CLS] question tokens [SEP] context tokens [SEP].\n",
    "        \"\"\"\n",
    "\n",
    "        # create question mask based on token_type_ids\n",
    "        # value is 0 for question tokens, 1 for context tokens\n",
    "        qmask = self.inputs['token_type_ids'].lt(1)\n",
    "        qt = torch.masked_select(self.inputs['input_ids'], qmask)\n",
    "        chunk_size = self.max_len - qt.size()[0] - 1 # the \"-1\" accounts for\n",
    "        # having to add an ending [SEP] token to the end\n",
    "\n",
    "        # create a dict of dicts; each sub-dict mimics the structure of pre-chunked model input\n",
    "        chunked_input = OrderedDict()\n",
    "        for k,v in self.inputs.items():\n",
    "            q = torch.masked_select(v, qmask)\n",
    "            c = torch.masked_select(v, ~qmask)\n",
    "            chunks = torch.split(c, chunk_size)\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                if i not in chunked_input:\n",
    "                    chunked_input[i] = {}\n",
    "\n",
    "                thing = torch.cat((q, chunk))\n",
    "                if i != len(chunks)-1:\n",
    "                    if k == 'input_ids':\n",
    "                        thing = torch.cat((thing, torch.tensor([102])))\n",
    "                    else:\n",
    "                        thing = torch.cat((thing, torch.tensor([1])))\n",
    "\n",
    "                chunked_input[i][k] = torch.unsqueeze(thing, dim=0)\n",
    "        return chunked_input\n",
    "\n",
    "    def get_answer(self):\n",
    "        if self.chunked:\n",
    "            answer = ''\n",
    "            for k, chunk in self.inputs.items():\n",
    "                a = self.model(**chunk)\n",
    "\n",
    "\n",
    "                answer_start_scores = a[0]\n",
    "                answer_end_scores = a[1]\n",
    "                answer_start = torch.argmax(answer_start_scores)\n",
    "                answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "                ans = self.convert_ids_to_string(chunk['input_ids'][0][answer_start:answer_end])\n",
    "                if ans != '[CLS]':\n",
    "                    answer += ans + \"  \"\n",
    "            return answer\n",
    "        else:\n",
    "            a = self.model(**self.inputs)\n",
    "\n",
    "\n",
    "            answer_start_scores = a[0]\n",
    "            answer_end_scores = a[1]      \n",
    "            answer_start = torch.argmax(answer_start_scores)  # get the most likely beginning of answer with the argmax of the score\n",
    "            answer_end = torch.argmax(answer_end_scores) + 1  # get the most likely end of answer with the argmax of the score\n",
    "        \n",
    "            return self.convert_ids_to_string(self.inputs['input_ids'][0][\n",
    "                                              answer_start:answer_end])\n",
    "\n",
    "    def convert_ids_to_string(self, input_ids):\n",
    "        return self.tokenizer.convert_tokens_to_string(self.tokenizer.convert_ids_to_tokens(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4vy8Y2nPyUg"
   },
   "source": [
    "Below is our clean, fully working QA system! Feel free to add your own questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FO1MJmscPyUh"
   },
   "outputs": [],
   "source": [
    "# collapse-hide \n",
    "\n",
    "# to make the following output more readable I'll turn off the token sequence length warning\n",
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NjRcgnn_PyUj",
    "outputId": "d077730f-c20b-48c0-99c7-8761b114ae39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is prime minister of india?\n",
      "Top wiki result: <WikipediaPage 'List of prime ministers of India'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (914 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: [CLS] Who is prime minister of india? [SEP] The Prime Minister of India is the chief executive of the Government of India / Narendra Modi / \n",
      "\n",
      "Question: Why is the sky blue?\n",
      "Top wiki result: <WikipediaPage 'Diffuse sky radiation'>\n",
      "Answer: Rayleigh scattering / \n",
      "\n",
      "Question: How many sides does a pentagon have?\n",
      "Top wiki result: <WikipediaPage 'The Pentagon'>\n",
      "Answer: five / \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    'Who is prime minister of india?',\n",
    "    'Why is the sky blue?',\n",
    "    'How many sides does a pentagon have?'\n",
    "]\n",
    "\n",
    "reader = DocumentReader(\"deepset/bert-base-cased-squad2\") \n",
    "\n",
    "# if you trained your own model using the training cell earlier, you can access it with this:\n",
    "#reader = DocumentReader(\"./models/bert/bbu_squad2\")\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    results = wiki.search(question)\n",
    "    \n",
    "    for i in results:\n",
    "        try:\n",
    "          wiki.page(i)\n",
    "        except:\n",
    "          continue\n",
    "        page = wiki.page(i)\n",
    "        break\n",
    "    print(f\"Top wiki result: {page}\")\n",
    "\n",
    "    text = page.content\n",
    "\n",
    "    reader.tokenize(question, text)\n",
    "    a = reader.get_answer()\n",
    "    \n",
    "    print(f\"Answer: {reader.get_answer()}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIDaPEPDPyUm"
   },
   "source": [
    "We're chunking a wiki article in such a way that we could be ending a chunk in the middle of a sentence -- Can we improve our `chunkify` method? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzPxrmcXPyUm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Research Assistant Yash.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
